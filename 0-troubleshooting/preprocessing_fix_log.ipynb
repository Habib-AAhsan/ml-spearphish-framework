{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6995d1bf",
   "metadata": {},
   "source": [
    "# üß† Preprocessing Fix Log & Troubleshooting Guide (NLTK / `preprocess.py`)\n",
    "\n",
    "This notebook documents the problems and solutions applied to make the `preprocess.py` script work reliably for text preprocessing in the `ml-spearphish-framework`.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Problem Summary\n",
    "\n",
    "The `preprocess.py` script was filtering **everything**, even valid words. The root cause was a broken `word_tokenize()` call from NLTK ‚Äî due to a bug in recent NLTK versions.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What We Did to Fix It\n",
    "\n",
    "### 1. Identified the Bug\n",
    "- NLTK threw this error:\n",
    "  ```\n",
    "  LookupError: Resource punkt_tab not found.\n",
    "  ```\n",
    "- Cause: `punkt_tab` is **not a real resource** ‚Äî this is a bug introduced in NLTK ‚â• 3.8.1.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Verified Tokenizer Error\n",
    "```bash\n",
    "python -c \"from nltk.tokenize import word_tokenize; print(word_tokenize('Hello World!'))\"\n",
    "```\n",
    "- Result: `LookupError` ‚Üí `punkt_tab` ‚Üí tokenizer broken\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Downgraded NLTK to Fix It\n",
    "```bash\n",
    "pip uninstall nltk -y\n",
    "pip install nltk==3.8.0\n",
    "```\n",
    "‚úÖ Fixes `punkt_tab` error permanently.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Downloaded `punkt.zip` Manually (SSL Fix)\n",
    "- macOS SSL blocked downloads.\n",
    "- So we downloaded from:\n",
    "  [punkt.zip (GitHub mirror)](https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/tokenizers/punkt.zip)\n",
    "- Extracted into:\n",
    "  ```\n",
    "  ./nltk_data/tokenizers/punkt/\n",
    "  ```\n",
    "\n",
    "‚úÖ `english.pickle` now available.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Updated Preprocessing Code to Use Local NLTK Path\n",
    "```python\n",
    "import nltk\n",
    "nltk.data.path.append('./nltk_data')\n",
    "```\n",
    "\n",
    "‚úÖ Tokenizer loads from the correct path.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Ran `preprocess.py`\n",
    "- Prompted:\n",
    "  ```\n",
    "  ‚û°Ô∏è Process full dataset? (y/n):\n",
    "  ```\n",
    "  - `n` = Test only first 10 rows\n",
    "  - `y` = Process full dataset and save\n",
    "\n",
    "‚úÖ Success with both modes.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Fixed Pandas Warning (Optional)\n",
    "To silence:\n",
    "```text\n",
    "SettingWithCopyWarning\n",
    "```\n",
    "\n",
    "Add:\n",
    "```python\n",
    "df = df.copy()\n",
    "```\n",
    "Before assigning:\n",
    "```python\n",
    "df[\"clean_text\"] = ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ What to Do If This Happens Again\n",
    "\n",
    "| Step | Action | Purpose |\n",
    "|------|--------|---------|\n",
    "| 1Ô∏è‚É£ | `python -c \"from nltk.tokenize import word_tokenize; print(word_tokenize('Hello World!'))\"` | Check if `punkt` works |\n",
    "| 2Ô∏è‚É£ | If error ‚Üí `pip install nltk==3.8.0` | Downgrade to stable |\n",
    "| 3Ô∏è‚É£ | If SSL issue ‚Üí manually download `punkt.zip` | Use offline |\n",
    "| 4Ô∏è‚É£ | Add `nltk.data.path.append('./nltk_data')` | Point to local data |\n",
    "| 5Ô∏è‚É£ | Test with `n`, run full with `y` | Confirm cleaning works |\n",
    "| 6Ô∏è‚É£ | Add `df = df.copy()` if warning shows | Avoid Pandas side effects |\n",
    "\n",
    "---\n",
    "\n",
    "## üßæ Add This to README.md or Project Log\n",
    "\n",
    "```text\n",
    "# nltk fix log\n",
    "- NLTK downgraded to 3.8.0\n",
    "- punkt manually installed in ./nltk_data/tokenizers/punkt/\n",
    "- nltk.data.path.append('./nltk_data') added to scripts\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ You are now ready for clean, stable, repeatable preprocessing every time.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
